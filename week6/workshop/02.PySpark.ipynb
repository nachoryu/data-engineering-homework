{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "457267a9-0886-40c1-9963-c4a15b4e8529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/16 18:13:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 전역 SparkContext 정리\n",
    "SparkContext.setSystemProperty('spark.driver.host', '127.0.0.1')\n",
    "SparkContext.setSystemProperty('spark.driver.bindAddress', '127.0.0.1')\n",
    "\n",
    "# 모든 활성 context 중지\n",
    "if SparkContext._active_spark_context:\n",
    "    SparkContext._active_spark_context.stop()\n",
    "\n",
    "# 새로 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c74ec564-aa61-4f08-9d74-866d41f181e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('fhvhv_tripdata_2021-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4899b3ec-f894-4e3d-9983-8686fc6b4a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', StringType(), True), StructField('DOLocationID', StringType(), True), StructField('SR_Flag', StringType(), True)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d28faa8-8e98-4344-97c9-3e603ffe4e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a321e9f-8246-4111-b136-1f81753283b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_csv('head.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73c1bed0-77de-44db-becc-89d88ba74ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('SR_Flag', DoubleType(), True)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0fa6b76-62b5-449f-8106-a6ecc509b1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92b1e408-092b-4be8-98d8-3705760285a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cc40084-4837-4b90-b9d4-b6928385807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b2630d0-6964-4fe0-92fb-c08ca157d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "473e8525-a73a-49dc-a164-6c482bcb0f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/16 18:15:48 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/02/16 18:15:50 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "26/02/16 18:15:51 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.write.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81075466-0613-46c8-9f8e-43e86043c662",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cfd550b-0a46-48ed-a74f-086733eae7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6420bde6-58da-46d4-968d-9b47df9b391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crazy_stuff(base_num):\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}'\n",
    "    elif num % 3 == 0:\n",
    "        return f'a/{num:03x}'\n",
    "    else:\n",
    "        return f'e/{num:03x}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fc228b2-73dc-4dd3-bf9e-44887c9139ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "crazy_stuff_udf = F.udf(crazy_stuff, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47bf2293-3d07-4ca4-a815-1d40f34b1524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+------------+------------+\n",
      "|base_id|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "|  e/9ce| 2021-01-03|  2021-01-03|         132|          91|\n",
      "|  e/9ce| 2021-01-01|  2021-01-01|         256|          65|\n",
      "|  e/9ce| 2021-01-03|  2021-01-03|         191|         191|\n",
      "|  e/9ce| 2021-01-03|  2021-01-03|         195|         265|\n",
      "|  e/9ce| 2021-01-03|  2021-01-03|         119|         241|\n",
      "|  s/acd| 2021-01-01|  2021-01-01|         148|          61|\n",
      "|  e/9ce| 2021-01-01|  2021-01-01|         119|           1|\n",
      "|  e/b32| 2021-01-01|  2021-01-01|         164|          68|\n",
      "|  e/b38| 2021-01-01|  2021-01-01|         220|         240|\n",
      "|  s/b13| 2021-01-03|  2021-01-03|          42|         254|\n",
      "|  e/b3e| 2021-01-04|  2021-01-04|          23|          23|\n",
      "|  e/b35| 2021-01-03|  2021-01-03|         251|         261|\n",
      "|  e/95b| 2021-01-05|  2021-01-05|          35|          76|\n",
      "|  e/b47| 2021-01-04|  2021-01-04|          72|         155|\n",
      "|  e/b3c| 2021-01-02|  2021-01-02|         119|         241|\n",
      "|  e/b14| 2021-01-02|  2021-01-02|           3|         174|\n",
      "|  e/b38| 2021-01-02|  2021-01-02|         113|         132|\n",
      "|  e/95b| 2021-01-04|  2021-01-04|         221|         221|\n",
      "|  s/acd| 2021-01-05|  2021-01-05|          95|          95|\n",
      "|  e/b35| 2021-01-03|  2021-01-03|         148|         256|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .withColumn('base_id', crazy_stuff_udf(df.dispatching_base_num)) \\\n",
    "    .select('base_id', 'pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a5f3e43-677a-4208-b987-6f051564057d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: int, DOLocationID: int]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    "  .filter(df.hvfhs_license_num == 'HV0003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee0d8f9-65dd-4aff-a98c-ce17d8d217a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
